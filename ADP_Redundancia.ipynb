{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ADP\n",
    "import data_manipulation as dm\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.covariance import EllipticEnvelope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = pickle.load(open(\"ADP_Iterations/var.pkl\", \"rb\"))\n",
    "\n",
    "gra_list = var['gra_list']\n",
    "iterations = var['iter']\n",
    "total = var['total']\n",
    "background_percent = var['back_percent']\n",
    "test_size = var['test_size']\n",
    "b_test = var['b_test']\n",
    "\n",
    "iterations = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(df):\n",
    "    s = np.sum(df, axis=0)\n",
    "    m = len(df)\n",
    "    mu = s/m\n",
    "    vr = np.sum((df - mu)**2, axis=0)\n",
    "    variance = vr/m\n",
    "    var_dia = np.diag(variance)\n",
    "    k = len(mu)\n",
    "    X = df - mu\n",
    "    p = 1/((2*np.pi)**(k/2)*(np.linalg.det(var_dia)**0.5))* np.exp(-0.5* np.sum(X @ np.linalg.pinv(var_dia) * X,axis=1))\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tpfpfn(ep, p, y):\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "    for i in range(len(y)):\n",
    "        if p[i] <= ep and y[i] == 1:\n",
    "            tp += 1\n",
    "        elif p[i] <= ep and y[i] == 0:\n",
    "            fp += 1\n",
    "        elif p[i] > ep and y[i] == 1:\n",
    "            fn += 1\n",
    "    return tp, fp, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(ep, p, y):\n",
    "    tp, fp, fn = tpfpfn(ep,p, y)\n",
    "    try:\n",
    "        prec = tp/(tp + fp)\n",
    "        rec = tp/(tp + fn)\n",
    "        f1 = 2*prec*rec/(prec + rec)\n",
    "    except:\n",
    "        f1 = 0\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_func(p, decision, L2):\n",
    "    eps = [i for i in p if i <= p.mean()]\n",
    "    f = [f1(i,p, decision) for i in eps]\n",
    "    e_idx = np.array(f).argmax()\n",
    "    e = eps[e_idx]\n",
    "    prob_label = []\n",
    "    for i in range(L2):\n",
    "        if p[i] <= e:\n",
    "            prob_label.append(1)\n",
    "        else:\n",
    "            prob_label.append(0)\n",
    "    return prob_label, e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder(static, streaming, reduction=11, epochs=200, batch_size=1024, layer1=42, layer2=21, activation='exponential'):\n",
    "    L, W = static.shape\n",
    "    \n",
    "    visible = Input(shape=(W,))\n",
    "    e = Dense(layer1)(visible)\n",
    "    e = BatchNormalization()(e)\n",
    "    e = LeakyReLU()(e)\n",
    "\n",
    "    e = Dense(layer2)(e)\n",
    "    e = BatchNormalization()(e)\n",
    "    e = LeakyReLU()(e)\n",
    "\n",
    "    n_bottleneck = round(reduction)\n",
    "    bottleneck = Dense(n_bottleneck)(e)\n",
    "\n",
    "    d = Dense(layer2)(bottleneck)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU()(d)\n",
    "\n",
    "    d = Dense(layer1)(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU()(d)\n",
    "\n",
    "    cp = tf.keras.callbacks.ModelCheckpoint(filepath=\"autoencoder_fraud.h5\",\n",
    "                               mode='min', monitor='val_loss', verbose=0, save_best_only=True)\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            min_delta=0.0001,\n",
    "            patience=10,\n",
    "            verbose=0, \n",
    "            mode='min',\n",
    "            restore_best_weights=True)\n",
    "    \n",
    "    output = Dense(W, activation=activation)(d)\n",
    "    model = Model(inputs=visible, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    history = model.fit(static, static, epochs=epochs, batch_size=batch_size, \n",
    "                        verbose=0, validation_data=(streaming, streaming),\n",
    "                    callbacks=[cp, early_stop])\n",
    "    \n",
    "    streaming_predictions = model.predict(streaming)\n",
    "    mse = np.mean(np.power(streaming - streaming_predictions, 2), axis=1)\n",
    "\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for n_i in range(iterations):\n",
    "    ##################################################################\n",
    "    # ------------------ LER OS DADOS DA ITERAÇÃO ------------------ #\n",
    "    # -------------------------------------------------------------- #\n",
    "    static = pickle.load(open(\"ADP_Iterations/Static_Iter{}.pkl\".format(n_i), \"rb\"))\n",
    "    streaming = pickle.load(open(\"ADP_Iterations/Streaming_Iter{}.pkl\".format(n_i), \"rb\"))\n",
    "    data = np.concatenate((static,streaming), axis=0)\n",
    "    L, W = data.shape\n",
    "    L1, _ = static.shape\n",
    "    L2, _ = streaming.shape\n",
    "    \n",
    "    ##################################################################\n",
    "    # ------------ DATAFRAME PARA METODO ANOMALY_CLOUDS ------------ #\n",
    "    # -------------------------------------------------------------- #\n",
    "    detection_info = pd.DataFrame(np.zeros((len(gra_list),6)), columns=['Granularity',\n",
    "                                                            'True_Positive', 'True_Negative',\n",
    "                                                            'False_Positive','False_Negative', \n",
    "                                                            'N_Groups'])\n",
    "    detection_info['Granularity'] = gra_list\n",
    "    \n",
    "    ##################################################################\n",
    "    # ------------ DATAFRAME PARA METODO PROBABILIDADE ------------- #\n",
    "    # -------------------------------------------------------------- #\n",
    "    prob_detection_info = pd.DataFrame(np.zeros((len(gra_list),6)), columns=['Granularity',\n",
    "                                                            'True_Positive', 'True_Negative',\n",
    "                                                            'False_Positive','False_Negative', \n",
    "                                                            'N_Groups'])\n",
    "    prob_detection_info['Granularity'] = gra_list\n",
    "    \n",
    "    \n",
    "    ##################################################################\n",
    "    # ------------- DATAFRAME PARA METODOS EM CASCATA -------------- #\n",
    "    # -------------------------------------------------------------- #\n",
    "    cascata_detection_info = pd.DataFrame(np.zeros((len(gra_list),6)), columns=['Granularity',\n",
    "                                                            'True_Positive', 'True_Negative',\n",
    "                                                            'False_Positive','False_Negative', \n",
    "                                                            'N_Groups'])\n",
    "    cascata_detection_info['Granularity'] = gra_list\n",
    "    \n",
    "\n",
    "    for gra in gra_list:\n",
    "        ##################################################################\n",
    "        # -------------------- LER OS DADOS DO ADP --------------------- #\n",
    "        # -------------------------------------------------------------- #\n",
    "        \n",
    "        output = pickle.load(open(\"ADP_Iterations/Output_Iter{}_Gra{}.pkl\".format(n_i, gra), \"rb\"))\n",
    "        \n",
    "        \n",
    "        ##################################################################\n",
    "        # ------------------- METODO ANOMALY CLOUDS -------------------- #\n",
    "        # -------------------------------------------------------------- #\n",
    "        on_center = output['centre']\n",
    "        on_IDX = output['IDX']\n",
    "        online_labels = output['IDX'][L1:]\n",
    "        \n",
    "        label = np.zeros((L2))\n",
    "        label[b_test:] = 1\n",
    "        decision = np.zeros((L2))\n",
    "    \n",
    "        cloud_info = pd.DataFrame(np.zeros((len(on_center),4)),columns=['Total_Samples','Old_Samples',\n",
    "                                                                        'Percentage_Old_Samples', 'Percentage_of_Samples'])\n",
    "\n",
    "        for j in range (len(on_IDX)):\n",
    "            if j < L1:\n",
    "                cloud_info.loc[int(on_IDX[j]),'Old_Samples'] += 1\n",
    "            cloud_info.loc[int(on_IDX[j]),'Total_Samples'] += 1\n",
    "\n",
    "        cloud_info.loc[:,'Percentage_Old_Samples'] = cloud_info.loc[:,'Old_Samples'] * 100 / cloud_info.loc[:,'Total_Samples']\n",
    "        cloud_info.loc[:,'Percentage_of_Samples'] = cloud_info.loc[:,'Total_Samples'] * 100/ cloud_info.loc[:,'Total_Samples'].sum()\n",
    "\n",
    "        anomaly_clouds=[]\n",
    "        n_anomalies = 0\n",
    "\n",
    "        for j in range(len(on_center)):\n",
    "            if cloud_info.loc[j,'Percentage_Old_Samples'] == 0 :\n",
    "                n_anomalies += cloud_info.loc[j,'Total_Samples']\n",
    "                anomaly_clouds.append(j)\n",
    "\n",
    "        if n_anomalies != 0:\n",
    "            for j in range(len(online_labels)): \n",
    "                if online_labels[j] in anomaly_clouds:\n",
    "                    decision[j] = 1\n",
    "\n",
    "        for j in range(len(label)):\n",
    "            if label[j] == 1:\n",
    "                if decision[j] == label[j]:\n",
    "                    detection_info.loc[gra-1,'True_Positive'] += 1\n",
    "                else:\n",
    "                    detection_info.loc[gra-1,'False_Negative'] += 1     \n",
    "            else:\n",
    "                if decision[j] == label[j]:\n",
    "                    detection_info.loc[gra-1,'True_Negative'] += 1\n",
    "                else:\n",
    "                    detection_info.loc[gra-1,'False_Positive'] += 1\n",
    "    \n",
    "        detection_info.loc[gra-1,'N_Groups'] = max(on_IDX) + 1\n",
    "        prob_detection_info.loc[gra-1,'N_Groups'] = max(on_IDX) + 1\n",
    "        cascata_detection_info.loc[gra-1,'N_Groups'] = max(on_IDX) + 1\n",
    "        \n",
    "        ##################################################################\n",
    "        # ------------- PROBABILIDADE EM RELAÇÃO AO GERAL -------------- #\n",
    "        # --------------------- ANOMALY CLOUDS ------------------------- #\n",
    "        # -------------------------------------------------------------- #\n",
    "        p = probability(streaming)\n",
    "        mse = autoencoder(static,streaming)\n",
    "        \n",
    "        plt.figure(figsize = [16,7])\n",
    "        plt.suptitle('GRA - {}'.format(gra),fontsize=25)\n",
    "        indices = [i for i in range(L2) if online_labels[i] in anomaly_clouds]\n",
    "        plt.title('Analise das amostras dentro dos Data Clouds')\n",
    "        plt.scatter(p.reshape(-1,1), mse.reshape(-1,1), label = 'Amostras', edgecolors='k')\n",
    "        plt.scatter(p.reshape(-1,1)[indices], mse.reshape(-1,1)[indices], c='r', label='Amostras Anomalas', edgecolors='k')\n",
    "        plt.xlabel('Probabilidade')\n",
    "        plt.ylabel('GlobalDensity')\n",
    "        plt.legend()\n",
    "        \n",
    "        prob_label, e = prob_func(p, decision, L2)\n",
    "                \n",
    "        for j in range(len(label)):\n",
    "            if label[j] == 1:\n",
    "                if prob_label[j] == label[j]:\n",
    "                    prob_detection_info.loc[gra-1,'True_Positive'] += 1\n",
    "                else:\n",
    "                    prob_detection_info.loc[gra-1,'False_Negative'] += 1     \n",
    "            else:\n",
    "                if prob_label[j] == label[j]:\n",
    "                    prob_detection_info.loc[gra-1,'True_Negative'] += 1\n",
    "                else:\n",
    "                    prob_detection_info.loc[gra-1,'False_Positive'] += 1\n",
    "                    \n",
    "                    \n",
    "        plt.plot([e, e],[min(mse), max(mse)], 'g', linewidth=3)\n",
    "        plt.savefig('Figures\\Anomaly_Clouds_{}_{}.png'.format(gra, n_i))\n",
    "        plt.show()\n",
    "        ##################################################################\n",
    "        # ------------- PROBABILIDADE VS GLOBAL DENSITY ---------------- #\n",
    "        # ------------------ BACKGROUND E SIGNAL ----------------------- #\n",
    "        # -------------------------------------------------------------- #\n",
    "        plt.figure(figsize = [16,7])\n",
    "        plt.suptitle('GRA - {}'.format(gra))\n",
    "        plt.title('Probabildiade vs GlobalDensity - Background e Signal')\n",
    "        plt.scatter(p.reshape(-1,1)[:b_test], mse.reshape(-1,1)[:b_test], label='Background', edgecolors='k')\n",
    "        plt.scatter(p.reshape(-1,1)[b_test:], mse.reshape(-1,1)[b_test:], c='r', label='Signal', edgecolors='k')\n",
    "        plt.xlabel('Probabilidade')\n",
    "        plt.ylabel('GlobalDensity')\n",
    "        plt.legend()\n",
    "        plt.plot([e, e], [min(mse), max(mse)], 'g', linewidth=3)\n",
    "        plt.savefig('Figures\\Background_Signal_{}_{}.png'.format(gra, n_i))\n",
    "        plt.show()\n",
    "        \n",
    "        cascata_label = prob_label * decision\n",
    "        \n",
    "        for j in range(len(label)):\n",
    "            if label[j] == 1:\n",
    "                if cascata_label[j] == label[j]:\n",
    "                    cascata_detection_info.loc[gra-1,'True_Positive'] += 1\n",
    "                else:\n",
    "                    cascata_detection_info.loc[gra-1,'False_Negative'] += 1     \n",
    "            else:\n",
    "                if cascata_label[j] == label[j]:\n",
    "                    cascata_detection_info.loc[gra-1,'True_Negative'] += 1\n",
    "                else:\n",
    "                    cascata_detection_info.loc[gra-1,'False_Positive'] += 1\n",
    "        \n",
    "        ##################################################################\n",
    "        # ------------------------- PLOT 3D ---------------------------- #\n",
    "        # ------------------ BACKGROUND E SIGNAL ----------------------- #\n",
    "        # -------------------------------------------------------------- #\n",
    "        \n",
    "        GD = output['SamplesGlobalDensity'][L1:]\n",
    "        fig = plt.figure(figsize = [16,7])\n",
    "        ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "        ax.scatter(p.reshape(-1,1)[:b_test], mse.reshape(-1,1)[:b_test], np.squeeze(np.array(GD))[:b_test], label='Background', edgecolors='k')\n",
    "        ax.scatter(p.reshape(-1,1)[b_test:], mse.reshape(-1,1)[b_test:], np.squeeze(np.array(GD))[b_test:], c='r', label='Signal', edgecolors='k')\n",
    "        ax.set_xlabel('Probabilidade')\n",
    "        ax.set_ylabel('MSE')\n",
    "        ax.set_zlabel('GlobalDensity')\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "        detection_info.to_csv('results/Anomaly_Clouds_detection_info_{}_{}.csv'.format(gra,n_i), index=False)\n",
    "        prob_detection_info.to_csv('results/Prob_detection_info_{}_{}.csv'.format(gra,n_i), index=False)\n",
    "        cascata_detection_info.to_csv('results/Cascata_detection_info_{}_{}.csv'.format(gra,n_i), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib\n",
    "fig = plt.figure(figsize = [16,7])\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter(p.reshape(-1,1)[:b_test], mse.reshape(-1,1)[:b_test], np.squeeze(np.array(GD))[:b_test], label='Background', edgecolors='k')\n",
    "ax.scatter(p.reshape(-1,1)[b_test:], mse.reshape(-1,1)[b_test:], np.squeeze(np.array(GD))[b_test:], c='r', label='Signal', edgecolors='k')\n",
    "ax.set_xlabel('Probabilidade')\n",
    "ax.set_ylabel('MSE')\n",
    "ax.set_zlabel('GlobalDensity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
