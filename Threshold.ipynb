{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ADP\n",
    "import data_manipulation as dm\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiation Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    ##########################################################\n",
    "    # ------------------------------------------------------ #\n",
    "    # --------------------- INITIATION --------------------- #\n",
    "    # ------------------------------------------------------ #\n",
    "    ##########################################################\n",
    "    ### Define User Variables ###\n",
    "\n",
    "    # List of Granularities\n",
    "    gra_list = [i for i in range(1,11)]\n",
    "\n",
    "    # Number of Iterations\n",
    "    iterations = 33\n",
    "\n",
    "    # Number of events\n",
    "    total = 10000\n",
    "\n",
    "    # Number of Data-set divisions\n",
    "    windows = 100\n",
    "\n",
    "    # Percentage of background samples on the testing phase\n",
    "    background_percent = 0.99\n",
    "\n",
    "    # Percentage of samples on the training phase\n",
    "    test_size = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    ##########################################################\n",
    "    # ------------------------------------------------------ #\n",
    "    # ----------------------- LOADING ---------------------- #\n",
    "    # ------------------------------------------------------ #\n",
    "    ##########################################################\n",
    "    # Firstly the model loads the background and signal data, \n",
    "    # then it removes the attributes first string line, which \n",
    "    # are the column names, in order to avoid NaN values in \n",
    "    # the array.\n",
    "\n",
    "    print('         ==== Commencing Initiation ====\\n')\n",
    "\n",
    "    ### Background    \n",
    "    b_name='Input_Background_1.csv'\n",
    "    background = np.genfromtxt(b_name, delimiter=',')\n",
    "    background = background[1:,:]\n",
    "    Lb, W = background.shape\n",
    "    print(\"     .Background Loaded...\" )\n",
    "    print(\"     .Background shape: {}\".format(background.shape))\n",
    "\n",
    "    ### Signal\n",
    "    s_name='Input_Signal_1.csv'\n",
    "    signal = np.genfromtxt(s_name, delimiter=',')\n",
    "    signal = signal[1:,:]\n",
    "    Ls, _ = signal.shape\n",
    "    print(\"     .Signal Loaded...\")\n",
    "    print(\"     .Signal shape: {}\\n\".format(signal.shape))\n",
    "\n",
    "    print('\\n          ==== Initiation Complete ====\\n')\n",
    "    print('=*='*17 )\n",
    "    print('      ==== Commencing Data Processing ====')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction Functions\n",
    "\n",
    "### Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(df):\n",
    "    s = np.sum(df, axis=0)\n",
    "    m = len(df)\n",
    "    mu = s/m\n",
    "    vr = np.sum((df - mu)**2, axis=0)\n",
    "    variance = vr/m\n",
    "    var_dia = np.diag(variance)\n",
    "    k = len(mu)\n",
    "    X = df - mu\n",
    "    p = 1/((2*np.pi)**(k/2)*(np.linalg.det(var_dia)**0.5))* np.exp(-0.5* np.sum(X @ np.linalg.pinv(var_dia) * X,axis=1))\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder(data, reduction=11, epochs=200, batch_size=1024, layer1=42, layer2=21, activation='exponential'):\n",
    "    L, W = data.shape\n",
    "    \n",
    "    visible = Input(shape=(W,))\n",
    "    e = Dense(layer1)(visible)\n",
    "    e = BatchNormalization()(e)\n",
    "    e = LeakyReLU()(e)\n",
    "\n",
    "    e = Dense(layer2)(e)\n",
    "    e = BatchNormalization()(e)\n",
    "    e = LeakyReLU()(e)\n",
    "\n",
    "    n_bottleneck = round(reduction)\n",
    "    bottleneck = Dense(n_bottleneck)(e)\n",
    "\n",
    "    d = Dense(layer2)(bottleneck)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU()(d)\n",
    "\n",
    "    d = Dense(layer1)(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU()(d)\n",
    "\n",
    "    cp = tf.keras.callbacks.ModelCheckpoint(filepath=\"autoencoder_fraud.h5\",\n",
    "                               mode='min', monitor='loss', verbose=0, save_best_only=True)\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='loss',\n",
    "            min_delta=0.0001,\n",
    "            patience=10,\n",
    "            verbose=0, \n",
    "            mode='min',\n",
    "            restore_best_weights=True)\n",
    "    \n",
    "    output = Dense(W, activation=activation)(d)\n",
    "    model = Model(inputs=visible, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    history = model.fit(data, data, epochs=epochs, batch_size=batch_size, \n",
    "                        verbose=0,\n",
    "                    callbacks=[cp, early_stop])\n",
    "    \n",
    "    predictions = model.predict(data)\n",
    "    rmse = np.sqrt(np.mean(np.power(data - predictions, 2), axis=1))\n",
    "    \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EDA_calc (data):\n",
    "    L, W = data.shape\n",
    "    EDA = np.zeros((L,6))\n",
    "    \n",
    "    dist = cdist(data, data, metric='euclidean')\n",
    "    EDA[:,0] = 1/np.sum(dist,axis=0)\n",
    "    CumulativeProximity = np.sum(dist**2,axis=0)\n",
    "    EDA[:,1] = L*np.sum(CumulativeProximity)/(2*CumulativeProximity)\n",
    "    \n",
    "    dist = cdist(data, data, metric='cosine')\n",
    "    EDA[:,2] = 1/np.sum(dist,axis=0)\n",
    "    CumulativeProximity = np.sum(dist**2,axis=0)\n",
    "    EDA[:,3] = L*np.sum(CumulativeProximity)/(2*CumulativeProximity)\n",
    "    \n",
    "    dist = cdist(data, data, metric='mahalanobis')\n",
    "    EDA[:,4] = 1/np.sum(dist,axis=0)\n",
    "    CumulativeProximity = np.sum(dist**2,axis=0)\n",
    "    EDA[:,5] = L*np.sum(CumulativeProximity)/(2*CumulativeProximity)\n",
    "    \n",
    "    return EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    threshold_list = []\n",
    "    \n",
    "    for n_i in range(iterations):\n",
    "        t_dict = {}\n",
    "        \n",
    "        ##########################################################\n",
    "        # ------------------------------------------------------ #\n",
    "        # ------------------- Preparing Data ------------------- #\n",
    "        # ------------------------------------------------------ #\n",
    "        ##########################################################\n",
    "        print('\\n     => Iteration Number', (n_i+1) )\n",
    "        \n",
    "        # Divide data-set\n",
    "        b_samples = int(total*background_percent)\n",
    "        s_samples = total - b_samples\n",
    "        \n",
    "        print('         .Dividing background and signal sub-sets')\n",
    "        _, divided_background = train_test_split(background, test_size=b_samples/Lb)\n",
    "        _, divided_signal = train_test_split(signal, test_size=s_samples/Ls)\n",
    "\n",
    "        print('         .Selecting Signal on the following porpotion:')\n",
    "        print('             .{}% Background samples'.format(int(background_percent*100)))\n",
    "        print('             .{}% Signal samples'.format(int((1-background_percent)*100)))\n",
    "        print('             .{:9d} of Background samples'.format(int(b_samples)) )\n",
    "        print('             .{:9d} of Signal samples)'.format(int(s_samples)))\n",
    "\n",
    "        # Concatenating Signal and the Test Background sub-set\n",
    "        streaming_data_raw = np.concatenate((divided_background,divided_signal), axis=0)\n",
    "        print(\"             .FullData shape: {}\\n\".format(streaming_data_raw.shape))\n",
    "\n",
    "        # Normalize Data\n",
    "        print('         .Normalizing Data')\n",
    "        streaming = normalize(streaming_data_raw,norm='max',axis=0)\n",
    "        \n",
    "        x1 = [i for i in range(1,b_samples+1)]\n",
    "        x2 = [i for i in range(b_samples+1,b_samples+s_samples+1)]\n",
    "        label = np.array(b_samples*[0] + s_samples*[1])\n",
    "        ##########################################################\n",
    "        # ------------------------------------------------------ #\n",
    "        # -------------------- Probability --------------------- #\n",
    "        # ------------------------------------------------------ #\n",
    "        ##########################################################\n",
    "        print('             .Extracting Probability')\n",
    "        p = probability(streaming)\n",
    "        \n",
    "        \n",
    "        t_dict['Probability_Min'] = np.min(p[b_samples:])\n",
    "        t_dict['Probability_Max'] = np.max(p[b_samples:])\n",
    "        t_dict['Probability_Mean'] = np.mean(p[b_samples:])\n",
    "        t_dict['Probability_Median'] = np.median(p[b_samples:])\n",
    "        \n",
    "\n",
    "        f, ax = plt.subplots(8,1,figsize=(16,8*7))\n",
    "        \n",
    "        ax[0].set_title('Event Probability - Iteration {}'.format(n_i+1), fontsize=20)\n",
    "        ax[0].scatter(x1, p.reshape(-1,1)[:b_samples], c='b', label='Background', edgecolors='k')\n",
    "        ax[0].scatter(x2, p.reshape(-1,1)[b_samples:], c='r', label='Signal', edgecolors='k')\n",
    "        \n",
    "        ax[0].plot(x1+x2, len(x1+x2)*[t_dict['Probability_Mean']], color='tab:pink', linewidth=3, linestyle='--', label='Mean')\n",
    "        ax[0].plot(x1+x2, len(x1+x2)*[t_dict['Probability_Min']], color='tab:red', linewidth=3, linestyle='--', label='Min')\n",
    "        ax[0].plot(x1+x2, len(x1+x2)*[t_dict['Probability_Max']], color='tab:orange', linewidth=3, linestyle='--', label='Max')\n",
    "        ax[0].plot(x1+x2, len(x1+x2)*[t_dict['Probability_Median']], color='tab:green',linewidth=3, linestyle='--', label='Median')\n",
    "        \n",
    "        ax[0].set_xlabel('Event Index', fontsize=15)\n",
    "        ax[0].set_ylabel('Probability', fontsize=15)\n",
    "        ax[0].legend()\n",
    "        ##########################################################\n",
    "        # ------------------------------------------------------ #\n",
    "        # -------------------- Autoencoder --------------------- #\n",
    "        # ------------------------------------------------------ #\n",
    "        ##########################################################\n",
    "        print('             .Extracting MSE from autoencoder')\n",
    "        rmse = autoencoder(streaming)\n",
    "        \n",
    "        t_dict['Autoencoder_Min'] = np.min(rmse[b_samples:])\n",
    "        t_dict['Autoencoder_Max'] = np.max(rmse[b_samples:])\n",
    "        t_dict['Autoencoder_Mean'] = np.mean(rmse[b_samples:])\n",
    "        t_dict['Autoencoder_Median'] = np.median(rmse[b_samples:])\n",
    "        \n",
    "        ax[1].set_title('Reconstuction Error - Iteration {}'.format(n_i+1), fontsize=20)\n",
    "        ax[1].scatter(x1, rmse.reshape(-1,1)[:b_samples], c='b', label='Background', edgecolors='k')\n",
    "        ax[1].scatter(x2, rmse.reshape(-1,1)[b_samples:], c='r', label='Signal', edgecolors='k')\n",
    "        \n",
    "        ax[1].plot(x1+x2, len(x1+x2)*[t_dict['Autoencoder_Mean']], color='tab:pink', linewidth=3, linestyle='--', label='Mean')\n",
    "        ax[1].plot(x1+x2, len(x1+x2)*[t_dict['Autoencoder_Min']], color='tab:red', linewidth=3, linestyle='--', label='Min')\n",
    "        ax[1].plot(x1+x2, len(x1+x2)*[t_dict['Autoencoder_Max']], color='tab:orange', linewidth=3, linestyle='--', label='Max')\n",
    "        ax[1].plot(x1+x2, len(x1+x2)*[t_dict['Autoencoder_Median']], color='tab:green',linewidth=3, linestyle='--', label='Median')\n",
    "        \n",
    "        ax[1].set_xlabel('Event Index', fontsize=15)\n",
    "        ax[1].set_ylabel('MSE', fontsize=15)\n",
    "        ax[1].legend()\n",
    "        \n",
    "        \n",
    "        ##########################################################\n",
    "        # ------------------------------------------------------ #\n",
    "        # ------------------------ EDAs ------------------------ #\n",
    "        # ------------------------------------------------------ #\n",
    "        ##########################################################\n",
    "        EDA_labels = ['EuclideanCentrality', 'EuclideanDensity',\n",
    "                      'CosineCentrality', 'CosineDensity',\n",
    "                      'MahalanobisCentrality', 'MahalanobisDensity']\n",
    "        \n",
    "        print('             .Extracting EDAs')\n",
    "        EDA = EDA_calc(streaming)\n",
    "        \n",
    "        for i in range(6):\n",
    "            t_dict['{}_Min'.format(EDA_labels[i])] = np.min(EDA[b_samples:,i])\n",
    "            t_dict['{}_Max'.format(EDA_labels[i])] = np.max(EDA[b_samples:,i])\n",
    "            t_dict['{}_Mean'.format(EDA_labels[i])] = np.mean(EDA[b_samples:,i])\n",
    "            t_dict['{}_Median'.format(EDA_labels[i])] = np.median(EDA[b_samples:,i])\n",
    "\n",
    "            ax[i+2].set_title('{} - Iteration {}'.format(EDA_labels[i], n_i+1), fontsize=20)\n",
    "            ax[i+2].scatter(x1, EDA[:b_samples,i], c='b', label='Background', edgecolors='k')\n",
    "            ax[i+2].scatter(x2, EDA[b_samples:,i], c='r', label='Signal', edgecolors='k')\n",
    "\n",
    "            ax[i+2].plot(x1+x2, len(x1+x2)*[t_dict['{}_Mean'.format(EDA_labels[i])]], color='tab:pink', linewidth=3, linestyle='--', label='Mean')\n",
    "            ax[i+2].plot(x1+x2, len(x1+x2)*[t_dict['{}_Min'.format(EDA_labels[i])]], color='tab:red', linewidth=3, linestyle='--', label='Min')\n",
    "            ax[i+2].plot(x1+x2, len(x1+x2)*[t_dict['{}_Max'.format(EDA_labels[i])]], color='tab:orange', linewidth=3, linestyle='--', label='Max')\n",
    "            ax[i+2].plot(x1+x2, len(x1+x2)*[t_dict['{}_Median'.format(EDA_labels[i])]], color='tab:green',linewidth=3, linestyle='--', label='Median')\n",
    "\n",
    "            ax[i+2].set_xlabel('Event Index', fontsize=15)\n",
    "            ax[i+2].set_ylabel(EDA_labels[i], fontsize=15)\n",
    "            ax[i+2].legend()\n",
    "            \n",
    "        plt.show()\n",
    "        threshold_list.append(t_dict)\n",
    "        \n",
    "        data = np.concatenate((p.reshape(-1,1), rmse.reshape(-1,1), EDA, label.reshape(-1,1)), axis=1)\n",
    "        columns = ['Probability', 'RMSE'] + EDA_labels + ['label']\n",
    "        aux = pd.DataFrame(data, columns=columns)\n",
    "        \n",
    "        sns.set(font_scale=1)\n",
    "        sns.pairplot(aux, hue='label', diag_kind=\"hist\", corner=True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(threshold_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
