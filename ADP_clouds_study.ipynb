{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    }
   ],
   "source": [
    "# Author: Alexandre Gramfort <alexandre.gramfort@inria.fr>\n",
    "#         Albert Thomas <albert.thomas@telecom-paristech.fr>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import time\n",
    "from sklearn import svm\n",
    "from sklearn.datasets import make_moons, make_blobs\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "print(__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import data_manipulation as dm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "import ADP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ADP_Offline_Granularity_Iteration_5th(static, streaming, gra, b_test, n_i):\n",
    "    begin = dm.datetime.now()\n",
    "\n",
    "    L1, W = static.shape\n",
    "    L2, _ = streaming.shape\n",
    "\n",
    "    ##################################\n",
    "    ##### ----- STATIC ADP ----- #####\n",
    "    ##### ---------------------- #####\n",
    "\n",
    "    Input1 = {'data': np.vstack((static,streaming)),\n",
    "             'granularity': gra,\n",
    "             'distancetype': 'euclidean'}\n",
    "    \n",
    "    Input2 = {'data': static,\n",
    "             'granularity': gra,\n",
    "             'distancetype': 'euclidean'}\n",
    "    \n",
    "    Input3 = {'data': streaming,\n",
    "             'granularity': gra,\n",
    "             'distancetype': 'euclidean'}\n",
    "            \n",
    "    ADP_output = ADP.ADP(Input1, 'Offline')\n",
    "    \n",
    "    # Computing the number of clouds\n",
    "    ADP_output['n_data_clouds'] = max(ADP_output['IDX']) + 1\n",
    "\n",
    "    ADP_static_output = ADP.ADP(Input2, 'Offline')\n",
    "    \n",
    "    # Computing the number of clouds\n",
    "    ADP_static_output['n_data_clouds'] = max(ADP_static_output['IDX']) + 1\n",
    "\n",
    "    ADP_streaming_output = ADP.ADP(Input3, 'Offline')\n",
    "\n",
    "    # Computing the number of clouds\n",
    "    ADP_streaming_output['n_data_clouds'] = max(ADP_streaming_output['IDX']) + 1\n",
    "    \n",
    "    Output = {'ADP_output': ADP_output,\n",
    "             'ADP_static_output': ADP_static_output,\n",
    "             'ADP_streaming_output': ADP_streaming_output}\n",
    "    \n",
    "    return Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    ##########################################################\n",
    "    # ------------------------------------------------------ #\n",
    "    # --------------------- INITIATION --------------------- #\n",
    "    # ------------------------------------------------------ #\n",
    "    ##########################################################\n",
    "    ### Define User Variables ###\n",
    "\n",
    "    # List of Granularities\n",
    "    gra_list = [i for i in range(1,11)]\n",
    "\n",
    "    # Number of Iterations\n",
    "    iterations = 1\n",
    "\n",
    "    # Number of events\n",
    "    total = 10000\n",
    "\n",
    "    # Number of Data-set divisions\n",
    "    windows = 100\n",
    "\n",
    "    # Percentage of background samples on the testing phase\n",
    "    background_percent = 0.99\n",
    "\n",
    "    # Percentage of samples on the training phase\n",
    "    test_size = 0.3\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ==== Commencing Initiation ====\n",
      "\n",
      "     .Background Loaded...\n",
      "     .Background shape: (543500, 21)\n",
      "     .Signal Loaded...\n",
      "     .Signal shape: (522467, 21)\n",
      "\n",
      "\n",
      "          ==== Initiation Complete ====\n",
      "\n",
      "=*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*=\n",
      "      ==== Commencing Data Processing ====\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    ##########################################################\n",
    "    # ------------------------------------------------------ #\n",
    "    # ----------------------- LOADING ---------------------- #\n",
    "    # ------------------------------------------------------ #\n",
    "    ##########################################################\n",
    "    # Firstly the model loads the background and signal data, \n",
    "    # then it removes the attributes first string line, which \n",
    "    # are the column names, in order to avoid NaN values in \n",
    "    # the array.\n",
    "\n",
    "    print('         ==== Commencing Initiation ====\\n')\n",
    "\n",
    "    ### Background    \n",
    "    b_name='Input_Background_1.csv'\n",
    "    background = np.genfromtxt(b_name, delimiter=',')\n",
    "    background = background[1:,:]\n",
    "    print(\"     .Background Loaded...\" )\n",
    "    print(\"     .Background shape: {}\".format(background.shape))\n",
    "\n",
    "    ### Signal\n",
    "    s_name='Input_Signal_1.csv'\n",
    "    signal = np.genfromtxt(s_name, delimiter=',')\n",
    "    signal = signal[1:,:]\n",
    "    print(\"     .Signal Loaded...\")\n",
    "    print(\"     .Signal shape: {}\\n\".format(signal.shape))\n",
    "\n",
    "    print('\\n          ==== Initiation Complete ====\\n')\n",
    "    print('=*='*17 )\n",
    "    print('      ==== Commencing Data Processing ====')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     => Iteration Number 1\n",
      "         .Dividing training and testing sub-sets\n",
      "         .Selecting Signal on the following porpotion:\n",
      "             .99% Background samples\n",
      "             .1% Signal samples\n",
      "             .     7000 of Background samples (Offline)\n",
      "             .     2970 of Background samples (Online)\n",
      "             .       30 of Signal samples (Online)\n",
      "             .Offline shape: (7000, 21)\n",
      "             .Online shape: (3000, 21)\n",
      "\n",
      "         .Normalizing Data\n",
      "             .Executing for granularities [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "\n",
      "\n",
      "             .Iter: 0 - Granularity: 1\n",
      "                 .ADP (5th Method)\n",
      "\n",
      "\n",
      "             .Iter: 0 - Granularity: 2\n",
      "                 .ADP (5th Method)\n",
      "\n",
      "\n",
      "             .Iter: 0 - Granularity: 3\n",
      "                 .ADP (5th Method)\n",
      "\n",
      "\n",
      "             .Iter: 0 - Granularity: 4\n",
      "                 .ADP (5th Method)\n",
      "\n",
      "\n",
      "             .Iter: 0 - Granularity: 5\n",
      "                 .ADP (5th Method)\n",
      "\n",
      "\n",
      "             .Iter: 0 - Granularity: 6\n",
      "                 .ADP (5th Method)\n",
      "\n",
      "\n",
      "             .Iter: 0 - Granularity: 7\n",
      "                 .ADP (5th Method)\n",
      "\n",
      "\n",
      "             .Iter: 0 - Granularity: 8\n",
      "                 .ADP (5th Method)\n",
      "\n",
      "\n",
      "             .Iter: 0 - Granularity: 9\n",
      "                 .ADP (5th Method)\n",
      "\n",
      "\n",
      "             .Iter: 0 - Granularity: 10\n",
      "                 .ADP (5th Method)\n",
      "\n",
      "        ====Data Processing Complete====\n",
      "\n",
      "=*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*=\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    for n_i in range(iterations):\n",
    "        print('\\n     => Iteration Number', (n_i+1) )\n",
    "\n",
    "        # Divide data-set into training and testing sub-sets\n",
    "        print('         .Dividing training and testing sub-sets')\n",
    "        divided_background, _ = dm.divide(background, windows, total)\n",
    "\n",
    "        test = int(total*test_size)\n",
    "        b_test = int(test*background_percent)\n",
    "        static_data_raw, background_test = train_test_split(divided_background, test_size=test_size, random_state=42)\n",
    "        background_test, _ = dm.divide(background_test, windows, b_test)\n",
    "\n",
    "        # Defining number of events Signal events on online phase.\n",
    "        signal_online_samples = int(test - b_test)\n",
    "        reduced_signal, _ = dm.divide(signal, windows, signal_online_samples)\n",
    "\n",
    "        print('         .Selecting Signal on the following porpotion:')\n",
    "        print('             .{}% Background samples'.format(int(background_percent*100)))\n",
    "        print('             .{}% Signal samples'.format(int((1-background_percent)*100)))\n",
    "        print('             .{:9d} of Background samples (Offline)'.format(int(total*(1-test_size))))\n",
    "        print('             .{:9d} of Background samples (Online)'.format(int(b_test)) )\n",
    "        print('             .{:9d} of Signal samples (Online)'.format(int(signal_online_samples)))\n",
    "\n",
    "        # Concatenating Signal and the Test Background sub-set\n",
    "        streaming_data_raw = np.concatenate((background_test,reduced_signal), axis=0)\n",
    "        print(\"             .Offline shape: {}\".format(static_data_raw.shape))\n",
    "        print(\"             .Online shape: {}\\n\".format(streaming_data_raw.shape))\n",
    "\n",
    "        # Normalize Data\n",
    "        print('         .Normalizing Data')\n",
    "        static_data = normalize(static_data_raw,norm='max',axis=0)\n",
    "        streaming_data = normalize(streaming_data_raw,norm='max',axis=0)\n",
    "        \n",
    "        ADP_outputs = {}\n",
    "        \n",
    "        print('             .Executing for granularities', gra_list)\n",
    "        for gra in gra_list:\n",
    "            print('\\n\\n             .Iter: {} - Granularity: {}'.format(n_i, gra))\n",
    "            print('                 .ADP (5th Method)')\n",
    "            output = ADP_Offline_Granularity_Iteration_5th(static_data, streaming_data, gra, b_test, n_i)\n",
    "            ADP_outputs ['granularity_'+str(gra)] = output\n",
    "\n",
    "    print('\\n        ====Data Processing Complete====\\n' )\n",
    "    print('=*='*17 ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(data):\n",
    "    L = len(data)\n",
    "    mu = np.mean(data, axis=0)\n",
    "    variance = np.mean((data - mu)**2, axis=0)/L\n",
    "    var_dia = np.diag(variance)\n",
    "    k = len(mu)\n",
    "    X = data - mu\n",
    "\n",
    "    if np.prod(variance) == 0:\n",
    "        p = np.array(L*[0])\n",
    "    else:\n",
    "        p = 1/((2*np.pi)**(k/2)*(np.linalg.det(var_dia)**0.5))* np.exp(-0.5* np.sum(X @ np.linalg.pinv(var_dia) * \n",
    "                                                                                    X,axis=1))\n",
    "    \n",
    "    #print('-----')\n",
    "    #print(mu.shape)\n",
    "    #print(variance.shape)\n",
    "    #print(var_dia.shape)\n",
    "    #print('var - ',variance)\n",
    "    #print('det - ', np.linalg.det(var_dia)**0.5)\n",
    "    #print('fat - ', 1/((2*np.pi)**(k/2)*(np.linalg.det(var_dia)**0.5)))\n",
    "    #print('sum - ',X @ np.linalg.pinv(var_dia) * X )\n",
    "    #print('pot - ', np.sum(X @ np.linalg.pinv(var_dia) * X,axis=1))\n",
    "    #print('exp - ', np.exp(-0.5* np.sum(X @ np.linalg.pinv(var_dia) * X,axis=1)) )\n",
    "    #print('p - ', p)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### divide data clouds data anda compute probability\n",
    "\n",
    "data_clouds_data = {}\n",
    "\n",
    "for gra in ADP_outputs.keys():\n",
    "    idx = ADP_outputs[gra]['ADP_streaming_output']['IDX']\n",
    "    u = np.unique(idx)\n",
    "    dc_dic = {}\n",
    "    for i in u:\n",
    "        data = []\n",
    "    \n",
    "        for j in range(len(idx)):\n",
    "            if idx[j] == i:\n",
    "                data.append(list(streaming_data[j]))\n",
    "        \n",
    "        dc_dic[i] = np.nan_to_num(probability(data))\n",
    "    data_clouds_data[gra] = dc_dic\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Robust covariance': {'True_Positive': 0, 'True_Negative': 0, 'False_Positive': 0, 'False_Negative': 0, 'Time': 0}, 'One-Class SVM': {'True_Positive': 30, 'True_Negative': 58, 'False_Positive': 2910, 'False_Negative': 0, 'Time': 0.029787540435791016}, 'Isolation Forest': {'True_Positive': 1, 'True_Negative': 2744, 'False_Positive': 224, 'False_Negative': 29, 'Time': 5.546315431594849}, 'Local Outlier Factor': {'True_Positive': 0, 'True_Negative': 2964, 'False_Positive': 4, 'False_Negative': 30, 'Time': 0.0399327278137207}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mathe\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_lof.py:265: UserWarning: n_neighbors (3) is greater than the total number of samples (2). n_neighbors will be set to (n_samples - 1) for estimation.\n",
      "  warnings.warn(\"n_neighbors (%s) is greater than the \"\n",
      "C:\\Users\\mathe\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_lof.py:265: UserWarning: n_neighbors (3) is greater than the total number of samples (2). n_neighbors will be set to (n_samples - 1) for estimation.\n",
      "  warnings.warn(\"n_neighbors (%s) is greater than the \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Robust covariance': {'True_Positive': 0, 'True_Negative': 0, 'False_Positive': 0, 'False_Negative': 0, 'Time': 0}, 'One-Class SVM': {'True_Positive': 27, 'True_Negative': 263, 'False_Positive': 2688, 'False_Negative': 3, 'Time': 0.05987191200256348}, 'Isolation Forest': {'True_Positive': 4, 'True_Negative': 2508, 'False_Positive': 443, 'False_Negative': 26, 'Time': 16.011354684829712}, 'Local Outlier Factor': {'True_Positive': 1, 'True_Negative': 2899, 'False_Positive': 52, 'False_Negative': 29, 'Time': 0.11822152137756348}}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-183-5fafd6b519ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m                 \u001b[1;31m#algorithm.fit(X)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m                 \u001b[1;31m#y_pred = algorithm.predict(X)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m                 \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m                 \u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my_pred\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_predict\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    738\u001b[0m         \"\"\"\n\u001b[0;32m    739\u001b[0m         \u001b[1;31m# override for transductive outlier detectors like LocalOulierFactor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 740\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    741\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_iforest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    298\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_samples_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_samples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m         \u001b[0mmax_depth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m         super()._fit(X, y, max_samples,\n\u001b[0m\u001b[0;32m    301\u001b[0m                      \u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                      sample_weight=sample_weight)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_bagging.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, max_samples, max_depth, sample_weight)\u001b[0m\n\u001b[0;32m    367\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_seeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseeds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 369\u001b[1;33m         all_results = Parallel(n_jobs=n_jobs, verbose=self.verbose,\n\u001b[0m\u001b[0;32m    370\u001b[0m                                \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parallel_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m             delayed(_parallel_build_estimators)(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1046\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1047\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1048\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1049\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    864\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    865\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 866\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    867\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    782\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    783\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 784\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    785\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_bagging.py\u001b[0m in \u001b[0;36m_parallel_build_estimators\u001b[1;34m(n_estimators, ensemble, X, y, sample_weight, seeds, total_n_estimators, verbose)\u001b[0m\n\u001b[0;32m    108\u001b[0m                 \u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnot_indices_mask\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1240\u001b[0m         \"\"\"\n\u001b[0;32m   1241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1242\u001b[1;33m         super().fit(\n\u001b[0m\u001b[0;32m   1243\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1244\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    154\u001b[0m             \u001b[0mcheck_X_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csc\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m             \u001b[0mcheck_y_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m             X, y = self._validate_data(X, y,\n\u001b[0m\u001b[0;32m    157\u001b[0m                                        validate_separately=(check_X_params,\n\u001b[0;32m    158\u001b[0m                                                             check_y_params))\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    427\u001b[0m                 \u001b[1;31m# :(\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m                 \u001b[0mcheck_X_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_y_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_separately\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m                 \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_X_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 644\u001b[1;33m             _assert_all_finite(array,\n\u001b[0m\u001b[0;32m    645\u001b[0m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0;32m    646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[0;32m     94\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[0;32m     95\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'infinity'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'NaN, infinity'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m     97\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m                     (type_err,\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "L2, _ = streaming_data.shape\n",
    "target = np.zeros(L2)\n",
    "target[b_test:] = 1\n",
    "\n",
    "### Make detections with the scikit methods\n",
    "\n",
    "# Example settings\n",
    "signal_fraction = 0.01\n",
    "\n",
    "anomaly_algorithms = [\n",
    "    (\"Robust covariance\", EllipticEnvelope(contamination=signal_fraction)),\n",
    "    (\"One-Class SVM\", svm.OneClassSVM(nu=signal_fraction, kernel=\"rbf\",\n",
    "                                      gamma=0.1)),\n",
    "    (\"Isolation Forest\", IsolationForest(contamination=signal_fraction,\n",
    "                                         random_state=42)),\n",
    "    (\"Local Outlier Factor\", LocalOutlierFactor(\n",
    "        n_neighbors=3, contamination=signal_fraction))]\n",
    "\n",
    "anomaly_algorithms = anomaly_algorithms[1:] # retirei o Robust Covariance pq tava dando um erro pela pouca variação dos\n",
    "                                            # eventos dentro de um grupo, depois podemos voltar com ele\n",
    "accuracy_dict = {}\n",
    "\n",
    "for gra in data_clouds_data:\n",
    "    acc_dict = {\"True_Positive\": 0,\n",
    "                \"True_Negative\": 0,\n",
    "                \"False_Positive\": 0,\n",
    "                \"False_Negative\": 0,\n",
    "                \"Time\": 0}\n",
    "    models_dict = {\"Robust covariance\":acc_dict.copy(),\n",
    "                   \"One-Class SVM\": acc_dict.copy(),\n",
    "                   \"Isolation Forest\": acc_dict.copy(),\n",
    "                   \"Local Outlier Factor\": acc_dict.copy()}\n",
    "\n",
    "    for i, dc in enumerate(data_clouds_data[gra]):\n",
    "        X = data_clouds_data[gra][dc].reshape(-1, 1)\n",
    "        if len(np.unique(X)) != 1:\n",
    "            for name, algorithm in anomaly_algorithms:\n",
    "                t0 = time.time()\n",
    "                #algorithm.fit(X)\n",
    "                #y_pred = algorithm.predict(X)\n",
    "                y_pred = algorithm.fit_predict(X)\n",
    "                \n",
    "                y_pred[y_pred == 1] = 0\n",
    "                y_pred[y_pred == -1] = 1\n",
    "                \n",
    "                y_target = target[ADP_outputs[gra]['ADP_streaming_output']['IDX'] == i]\n",
    "                \n",
    "                tn, fp, fn, tp = confusion_matrix(y_target, y_pred, labels=[0,1]).ravel()\n",
    "                \n",
    "                models_dict[name][\"True_Positive\"] += tp\n",
    "                models_dict[name][\"True_Negative\"] += tn\n",
    "                models_dict[name][\"False_Positive\"] += fp\n",
    "                models_dict[name][\"False_Negative\"] += fn\n",
    "                \n",
    "                \n",
    "                t1 = time.time()\n",
    "                models_dict[name][\"Time\"] += t1-t0\n",
    "                \n",
    "    accuracy_dict[gra] = models_dict.copy()\n",
    "    print(models_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'granularity_1': {'One-Class SVM': {'True_Positive': 30,\n",
       "   'True_Negative': 58,\n",
       "   'False_Positive': 2910,\n",
       "   'False_Negative': 0,\n",
       "   'Time': 0.029009103775024414},\n",
       "  'Isolation Forest': {'True_Positive': 1,\n",
       "   'True_Negative': 2744,\n",
       "   'False_Positive': 224,\n",
       "   'False_Negative': 29,\n",
       "   'Time': 5.822308778762817},\n",
       "  'Local Outlier Factor': {'True_Positive': 0,\n",
       "   'True_Negative': 2964,\n",
       "   'False_Positive': 4,\n",
       "   'False_Negative': 30,\n",
       "   'Time': 0.04284811019897461}},\n",
       " 'granularity_2': {'One-Class SVM': {'True_Positive': 27,\n",
       "   'True_Negative': 263,\n",
       "   'False_Positive': 2688,\n",
       "   'False_Negative': 3,\n",
       "   'Time': 0.06191539764404297},\n",
       "  'Isolation Forest': {'True_Positive': 4,\n",
       "   'True_Negative': 2508,\n",
       "   'False_Positive': 443,\n",
       "   'False_Negative': 26,\n",
       "   'Time': 17.870941162109375},\n",
       "  'Local Outlier Factor': {'True_Positive': 1,\n",
       "   'True_Negative': 2899,\n",
       "   'False_Positive': 52,\n",
       "   'False_Negative': 29,\n",
       "   'Time': 0.13837218284606934}}}"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "506.4px",
    "left": "1166px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
